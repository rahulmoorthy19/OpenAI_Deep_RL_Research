{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q59vVp8zDh4B"
   },
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gH-o2fFfDlcQ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RgACFJPLG_R"
   },
   "source": [
    "## Implementation Details\n",
    "1. Initialize the Parameters of Actor Network and Critic Network\n",
    "2. For number of episodes:\n",
    "  1. Sample Reward by taking a step from state s to s1\n",
    "  2. The new loss minimization for vanilla policy gradients is log * advantage funtion\n",
    "    1. Advantage Function= r+V(s1)-V(s)\n",
    "  3. Traditional updates for the Q networks which uses the policy actions as it's target value\n",
    "\n",
    "  Details taken from [link](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_NHnQMSPKBS"
   },
   "source": [
    "# A2C Vanilla Policy Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCYyfJljDwPQ"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, state_size],name=\"input_state\")\n",
    "action_space=tf.compat.v1.placeholder(tf.int32, shape=[None, actions],name=\"action_space\")\n",
    "advantage_function=tf.compat.v1.placeholder(tf.float32, shape=[None,],name=\"advantage_funtion\")\n",
    "fc1=tf.keras.layers.Dense(10,activation='relu',name=\"fc1\")(input_state)\n",
    "fc2=tf.keras.layers.Dense(actions,activation='relu',name=\"fc2\")(fc1)\n",
    "fc3=tf.keras.layers.Dense(actions,name=\"fc3\")(fc2)\n",
    "action_output=tf.keras.layers.Softmax(name=\"action_output\")(fc3)\n",
    "neg_loss_prob=tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = action_space)\n",
    "loss=tf.math.reduce_mean(advantage_function*neg_loss_prob)\n",
    "training=tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
