{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q59vVp8zDh4B",
        "colab_type": "text"
      },
      "source": [
        "# Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-o2fFfDlcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from skimage.color import rgb2gray\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RgACFJPLG_R",
        "colab_type": "text"
      },
      "source": [
        "## Implementation Details\n",
        "1. Initialize the Parameters of Actor Network and Critic Network\n",
        "2. For number of episodes:\n",
        "  1. Sample Reward by taking a step from state s to s1\n",
        "  2. The new loss minimization for vanilla policy gradients is log * advantage funtion\n",
        "    1. Advantage Function= r+V(s1)-V(s)\n",
        "  3. Traditional updates for the Q networks which uses the policy actions as it's target value\n",
        "\n",
        "  Details taken from [link](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5cZwiIVnrhr",
        "colab_type": "text"
      },
      "source": [
        "# Setting up game environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNOvMAE7nzHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "env = env.unwrapped\n",
        "env.seed(1)\n",
        "state = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siRu-FDelW2F",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyu7r1XElaKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_number=env.action_space.n\n",
        "critic_learning_rate=0.00025\n",
        "action_learning_rate=0.00025      \n",
        "total_episodes = 50\n",
        "gamma=0.9\n",
        "state_size = [84, 84, 4]\n",
        "stack_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ4CTtIlnQC7",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzGzdCw7nOlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frame_preprocessing(image_frame):\n",
        "  gray = rgb2gray(image_frame)\n",
        "  normalized_frame = gray/255.0\n",
        "  preprocessed_frame = resize(normalized_frame, [84,84])\n",
        "  return preprocessed_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e185q2Qt_qso",
        "colab_type": "text"
      },
      "source": [
        "# Stacking Frames so that difference in the frames can be observed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgOosRRI_hiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    frame=frame_preprocessing(state)\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NHnQMSPKBS",
        "colab_type": "text"
      },
      "source": [
        "# A2C Vanilla Policy Actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCYyfJljDwPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b2361b1e-674e-4e2b-f93b-a6ff027fc3f6"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "actor_input_state=tf.compat.v1.placeholder(tf.float32, shape=[None,*state_size],name=\"actor_input_state\")\n",
        "actor_action_space=tf.compat.v1.placeholder(tf.int32, shape=[None, action_number],name=\"actor_action_space\")\n",
        "actor_advantage_function=tf.compat.v1.placeholder(tf.float32, shape=[None,],name=\"actor_advantage_funtion\")\n",
        "actor_cnn_layer_1=tf.keras.layers.Conv2D(filters=16,kernel_size=(8,8),strides=(4,4),activation=\"relu\")(actor_input_state)\n",
        "actor_cnn_layer_2=tf.keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(2,2),activation=\"relu\")(actor_cnn_layer_1)\n",
        "actor_flatten_layer=tf.keras.layers.Flatten()(actor_cnn_layer_2)\n",
        "actor_fully_connected_layer_1=tf.keras.layers.Dense(256,activation='relu',name=\"actor_fully_connected_layer_1\")(actor_flatten_layer)\n",
        "actor_output_layer=tf.keras.layers.Dense(action_number,name=\"actor_output_layer\")(actor_fully_connected_layer_1)\n",
        "actor_action_probability=tf.keras.layers.Softmax(name=\"actor_action_probability\")(actor_output_layer)\n",
        "actor_neg_loss_prob=tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = actor_output_layer, labels = actor_action_space)\n",
        "actor_loss=tf.math.reduce_mean(actor_advantage_function*actor_neg_loss_prob)\n",
        "actor_training=tf.compat.v1.train.AdamOptimizer(action_learning_rate).minimize(actor_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egl6zPAokbsJ",
        "colab_type": "text"
      },
      "source": [
        "# A2C Value Network Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnOY9mH_khP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "critic_input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, *state_size],name=\"critic_input_state\")\n",
        "critic_value_target=tf.compat.v1.placeholder(tf.float32, shape=[None],name=\"critic_value_target\")\n",
        "critic_action_space=tf.compat.v1.placeholder(tf.float32, shape=[None, action_number],name=\"critic_action_space\")\n",
        "critic_cnn_layer_1=tf.keras.layers.Conv2D(filters=16,kernel_size=(8,8),strides=(4,4),activation=\"relu\")(critic_input_state)\n",
        "critic_cnn_layer_2=tf.keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(2,2),activation=\"relu\")(critic_cnn_layer_1)\n",
        "critic_flatten_layer=tf.keras.layers.Flatten()(critic_cnn_layer_2)\n",
        "critic_fully_connected_layer_1=tf.keras.layers.Dense(256,activation='relu',name=\"critic_fully_connected_layer_1\")(critic_flatten_layer)\n",
        "critic_output_layer=tf.keras.layers.Dense(action_number,name=\"critic_output_layer\")(critic_fully_connected_layer_1)\n",
        "# critic_action_output=tf.keras.layers.Softmax(name=\"critic_action_output\")(critic_output_layer)\n",
        "critic_Q_value=tf.math.reduce_sum(tf.math.multiply(critic_output_layer, critic_action_space))\n",
        "critic_loss=tf.math.reduce_mean(tf.math.square(critic_value_target-critic_Q_value))\n",
        "critic_training=tf.compat.v1.train.AdamOptimizer(critic_learning_rate).minimize(critic_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RELliiotvj1",
        "colab_type": "text"
      },
      "source": [
        "# Integration of Actor and Critic to form A2C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Fb4ad5tuU9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "29bb7d5e-d687-48cf-bc90-adfddd318ccf"
      },
      "source": [
        "saver = tf.compat.v1.train.Saver()\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  for i in range(total_episodes):\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    while True:\n",
        "      action_probability=sess.run(actor_action_probability,feed_dict={actor_input_state:state.reshape((1, *state.shape))})\n",
        "      action = np.random.choice(range(action_probability.shape[1]), p=action_probability.ravel())\n",
        "      action_ = np.zeros(action_number)\n",
        "      action_[action] = 1\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      episode_rewards.append(reward)\n",
        "      if done:\n",
        "        total_reward = np.sum(episode_rewards)\n",
        "        print('Episode: {}'.format(i),\n",
        "              'Total reward: {}'.format(total_reward))\n",
        "        break\n",
        "      next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "      present_state_value=sess.run(critic_output_layer,feed_dict={critic_input_state:state.reshape((1,*state.shape))})\n",
        "      next_state_value=sess.run(critic_output_layer,feed_dict={critic_input_state:next_state.reshape((1,*state.shape))})\n",
        "      actor_adv_func=reward+(gamma*np.max(next_state_value))-np.max(present_state_value)\n",
        "      critic_update=reward+(gamma*np.max(next_state_value))\n",
        "      agent_training,agent_loss=sess.run([actor_training,actor_loss],feed_dict={actor_input_state:next_state.reshape((1,*state.shape)),\n",
        "                                                                                actor_action_space:action_.reshape((1,6)),\n",
        "                                                                                actor_advantage_function:np.array([actor_adv_func])})\n",
        "      teacher_training,teacher_loss=sess.run([critic_training,critic_loss],feed_dict={critic_input_state:next_state.reshape((1,*state.shape)),\n",
        "                                                                                critic_action_space:action_.reshape((1,6)),\n",
        "                                                                                critic_value_target:np.array([critic_update])})\n",
        "      state=next_state\n",
        "    if i % 10 == 0:\n",
        "      saver.save(sess, \"./models/model.ckpt\")\n",
        "      print(\"Model saved\")  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Total reward: 300.0\n",
            "Model saved\n",
            "Episode: 1 Total reward: 80.0\n",
            "Episode: 2 Total reward: 135.0\n",
            "Episode: 3 Total reward: 210.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLT-nnNR8bPr",
        "colab_type": "text"
      },
      "source": [
        "# Testing model snippey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULf7lFGs6MHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    total_test_rewards = []\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "    \n",
        "    for episode in range(1):\n",
        "        total_rewards = 0\n",
        "        \n",
        "        state = env.reset()\n",
        "        \n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode)\n",
        "        \n",
        "        while True:\n",
        "          state=frame_preprocessing(state)\n",
        "          action_probability=sess.run(actor_action_probability,feed_dict={actor_input_state:state.reshape((1,84,84,1))})\n",
        "          action = np.random.choice(range(action_probability.shape[1]), p=action_probability.ravel())\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "          total_rewards += reward\n",
        "          if done:\n",
        "            print (\"Score\", total_rewards)\n",
        "            total_test_rewards.append(total_rewards)\n",
        "            break    \n",
        "          state = next_state\n",
        "            \n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}