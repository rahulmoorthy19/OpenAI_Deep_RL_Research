{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q59vVp8zDh4B",
        "colab_type": "text"
      },
      "source": [
        "# Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-o2fFfDlcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RgACFJPLG_R",
        "colab_type": "text"
      },
      "source": [
        "## Implementation Details\n",
        "1. Initialize the Parameters of Actor Network and Critic Network\n",
        "2. For number of episodes:\n",
        "  1. Sample Reward by taking a step from state s to s1\n",
        "  2. The new loss minimization for vanilla policy gradients is log * advantage funtion\n",
        "    1. Advantage Function= r+V(s1)-V(s)\n",
        "  3. Traditional updates for the Q networks which uses the policy actions as it's target value\n",
        "\n",
        "  Details taken from [link](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NHnQMSPKBS",
        "colab_type": "text"
      },
      "source": [
        "# A2C Vanilla Policy Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCYyfJljDwPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, state_size],name=\"input_state\")\n",
        "action_space=tf.compat.v1.placeholder(tf.int32, shape=[None, actions],name=\"action_space\")\n",
        "advantage_function=tf.compat.v1.placeholder(tf.float32, shape=[None,],name=\"advantage_funtion\")\n",
        "fc1=tf.keras.layers.Dense(10,activation='relu',name=\"fc1\")(input_state)\n",
        "fc2=tf.keras.layers.Dense(actions,activation='relu',name=\"fc2\")(fc1)\n",
        "fc3=tf.keras.layers.Dense(actions,name=\"fc3\")(fc2)\n",
        "action_output=tf.keras.layers.Softmax(name=\"action_output\")(fc3)\n",
        "neg_loss_prob=tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = action_space)\n",
        "loss=tf.math.reduce_mean(advantage_function*neg_loss_prob)\n",
        "training=tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}