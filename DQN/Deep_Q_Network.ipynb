{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBAjdID8EvR",
        "colab_type": "code",
        "outputId": "ae20ab5e-0803-4082-a523-cf0e0187ee99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!pip install gym "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.3)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3PWIxzv8M_Q",
        "colab_type": "code",
        "outputId": "6f63a483-f123-483b-f5a4-b8a2bef0818b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!pip install atari-py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from atari-py) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM-WXJHT8Vgk",
        "colab_type": "code",
        "outputId": "a947446a-8f22-45df-fb58-3d58af20b5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip install gym[atari]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1SbGSl08dpV",
        "colab_type": "text"
      },
      "source": [
        "# Implementation details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aPsv_Uw8gnW",
        "colab_type": "text"
      },
      "source": [
        "There are 5 modules of Deep Q learning-\n",
        "1. CNN for interacting with the environment\n",
        "2. Experience replay so that gradient descent converges rather than diverges. It basically provides a sense of direction to where to move for reducing the loss\n",
        "3. A module to integrate both of the above modules and building a linking peice to complete the algorithm\n",
        "4. testing with the OpenAI gym environment\n",
        "5. Preprocessing of image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhN9HfBa8m8W",
        "colab_type": "text"
      },
      "source": [
        "# import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-c4ry1e8hw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from skimage.color import rgb2gray\n",
        "from collections import deque\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s60GirGLC2vZ",
        "colab_type": "text"
      },
      "source": [
        "# Initializing Gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MM5qEoK_elV",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "env = env.unwrapped\n",
        "env.seed(1)\n",
        "state = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku2QVDzSotY",
        "colab_type": "text"
      },
      "source": [
        "# Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74I-fefRSpob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_number=env.action_space.n\n",
        "learning_rate=0.00025\n",
        "batch_size=64\n",
        "stack_size = 4\n",
        "pretrain_length = batch_size   \n",
        "memory_size = 1000000         \n",
        "total_episodes = 50\n",
        "gamma=0.9\n",
        "state_size = [84, 84, 4]  \n",
        "explore_start = 1.0            \n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLNN1h3ASUL",
        "colab_type": "text"
      },
      "source": [
        "# Pre processing of image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HzU9oweAEaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frame_preprocessing(image_frame):\n",
        "  gray = rgb2gray(image_frame)\n",
        "  normalized_frame = gray/255.0\n",
        "  preprocessed_frame = resize(normalized_frame, [84,84])\n",
        "  return preprocessed_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl3EuAquEERE",
        "colab_type": "text"
      },
      "source": [
        "# Experience replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH5syw0iRZPn",
        "colab_type": "text"
      },
      "source": [
        "## Stacking frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1958wlWRYP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    frame=frame_preprocessing(state)\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSvGn-U1DHrL",
        "colab_type": "text"
      },
      "source": [
        "## Memory class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paUaFalaC9iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "        \n",
        "        return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx1hqJEnRt0m",
        "colab_type": "text"
      },
      "source": [
        "## Dealing with empty memory problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbWWj0GdEhHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = Memory(max_size = memory_size)\n",
        "for i in range(pretrain_length):\n",
        "    if i == 0:\n",
        "        state = env.reset()\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    #possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "    action = random.randint(1,env.action_space.n)-1\n",
        "    #action = possible_actions[choice]\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    #env.render()\n",
        "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "    if done:\n",
        "        next_state = np.zeros(state.shape)\n",
        "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "        action = possible_actions[action]\n",
        "        print(action.shape)\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        state = env.reset()\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "    else:\n",
        "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "        action = possible_actions[action]\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ThREIxSFHcG",
        "colab_type": "text"
      },
      "source": [
        "# Q Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5by7mO9GNIE",
        "colab_type": "text"
      },
      "source": [
        "The Q network architecture-\n",
        "1. 16 filters with 8 X 8 kernel and 4 strides with relu activation\n",
        "2. 32 filters with 4 X 4 kernel and 2 strides with relu activation\n",
        "3. 256 fully connected layer\n",
        "4. linear layer with number of actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQGq6ksIE5KD",
        "colab_type": "code",
        "outputId": "86599dbf-aead-4f9d-a3d6-c9662112b5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, *state_size],name=\"input_state\")\n",
        "y_j=tf.compat.v1.placeholder(tf.float32, shape=[None],name=\"y_j\")\n",
        "action_space=tf.compat.v1.placeholder(tf.float32, shape=[None, action_number],name=\"action_space\")\n",
        "cnn_layer_1=tf.keras.layers.Conv2D(filters=16,kernel_size=(8,8),strides=(4,4),activation=\"relu\")(input_state)\n",
        "cnn_layer_2=tf.keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(2,2),activation=\"relu\")(cnn_layer_1)\n",
        "flatten_layer=tf.keras.layers.Flatten()(cnn_layer_2)\n",
        "fully_connected_layer_1=tf.keras.layers.Dense(256,activation='relu',name=\"fully_connected_layer_1\")(flatten_layer)\n",
        "output_layer=tf.keras.layers.Dense(action_number,name=\"output_layer\")(fully_connected_layer_1)\n",
        "action_output=tf.keras.layers.Softmax(name=\"action_output\")(output_layer)\n",
        "Q_value=tf.math.reduce_sum(tf.math.multiply(output_layer, action_space))\n",
        "loss=tf.math.reduce_mean(tf.math.square(y_j-Q_value))\n",
        "training=tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko5UiAbHVG04",
        "colab_type": "text"
      },
      "source": [
        "# Training Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ny4l6-LMGCc",
        "colab_type": "code",
        "outputId": "b6e5eb01-67d3-4bd1-c063-ee76147191fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "saver = tf.compat.v1.train.Saver()\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    decay_step = 0\n",
        "    for i in range(total_episodes):\n",
        "      episode_rewards = []\n",
        "      state = env.reset()\n",
        "      #env.render()\n",
        "      state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "      while True:\n",
        "        epsilon=random.random()\n",
        "        decay_step +=1\n",
        "        explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "        if (explore_probability > epsilon):\n",
        "          action = random.randint(1,env.action_space.n)-1\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "        else:\n",
        "          action_probability=sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
        "          action = np.random.choice(range(action_probability.shape[1]), p=action_probability.ravel())\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        if done:\n",
        "          next_state = np.zeros(state.shape)\n",
        "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "          action = possible_actions[action]\n",
        "          memory.add((state, action, reward, next_state, done))\n",
        "          total_reward = np.sum(episode_rewards)\n",
        "          print('Episode: {}'.format(i),\n",
        "                                  'Total reward: {}'.format(total_reward),\n",
        "                                  'Explore P: {:.4f}'.format(explore_probability))\n",
        "          break\n",
        "        else:\n",
        "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "          action = possible_actions[action]\n",
        "          memory.add((state, action, reward, next_state, done))\n",
        "          state=next_state\n",
        "        batch = memory.sample(batch_size)\n",
        "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "        actions_mb = np.array([each[1] for each in batch])\n",
        "        rewards_mb = np.array([each[2] for each in batch]) \n",
        "        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "        dones_mb = np.array([each[4] for each in batch])\n",
        "        target_Qs_batch = []\n",
        "        Qs_next_state = sess.run(output_layer, feed_dict = {input_state: next_states_mb})\n",
        "        for j in range(len(batch)):\n",
        "          terminal = dones_mb[i]\n",
        "          if terminal:\n",
        "            target_Qs_batch.append(rewards_mb[i])\n",
        "          else:\n",
        "            target_Qs_batch.append(rewards_mb[i]+(gamma*np.max(Qs_next_state[i])))\n",
        "        targets_mb = np.array([each for each in target_Qs_batch])\n",
        "        loss_value, _ = sess.run([loss, training],\n",
        "                           feed_dict={input_state: states_mb,\n",
        "                                      y_j: targets_mb, \n",
        "                                      action_space: actions_mb})\n",
        "      if i % 5 == 0:\n",
        "        save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "        print(\"Model Saved\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Total reward: 180.0 Explore P: 0.9934\n",
            "Model Saved\n",
            "Episode: 1 Total reward: 315.0 Explore P: 0.9795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7218365496ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m                            feed_dict={input_state: states_mb,\n\u001b[1;32m     55\u001b[0m                                       \u001b[0my_j\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                       action_space: actions_mb})\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcHumO9GuZ7M",
        "colab_type": "text"
      },
      "source": [
        "# Testing of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOnw6YeojM1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    total_test_rewards = []\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "    \n",
        "    for episode in range(1):\n",
        "        total_rewards = 0\n",
        "        \n",
        "        state = env.reset()\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode)\n",
        "        \n",
        "        while True:\n",
        "            Qs = sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
        "            action = np.random.choice(range(Qs.shape[1]), p=Qs.ravel())\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            env.render()\n",
        "            total_rewards += reward\n",
        "            if done:\n",
        "                print (\"Score\", total_rewards)\n",
        "                total_test_rewards.append(total_rewards)\n",
        "                break    \n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            state = next_state\n",
        "            \n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}